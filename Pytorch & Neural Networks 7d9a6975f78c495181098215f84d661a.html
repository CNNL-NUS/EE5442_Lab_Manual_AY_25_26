<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Pytorch &amp; Neural Networks</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

/* Override strong tags inside headings to maintain consistent weight */
h1 strong,
h2 strong,
h3 strong {
	font-weight: 600;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 10px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.collection-content td {
	white-space: pre-wrap;
	word-break: break-word;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.callout img.notion-static-icon {
	width: 1em;
	height: 1em;
}

.callout p {
	margin: 0;
}

.callout h1,
.callout h2,
.callout h3 {
	margin: 0 0 0.6rem;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

blockquote.quote-large {
	font-size: 1.25em;
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", "SF Arabic", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", "SF Arabic", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", "SF Arabic", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", "SF Arabic", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", "SF Arabic", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray {
	color: rgba(125, 122, 117, 1);
	fill: rgba(125, 122, 117, 1);
}
.highlight-brown {
	color: rgba(159, 118, 90, 1);
	fill: rgba(159, 118, 90, 1);
}
.highlight-orange {
	color: rgba(210, 123, 45, 1);
	fill: rgba(210, 123, 45, 1);
}
.highlight-yellow {
	color: rgba(203, 148, 52, 1);
	fill: rgba(203, 148, 52, 1);
}
.highlight-teal {
	color: rgba(80, 148, 110, 1);
	fill: rgba(80, 148, 110, 1);
}
.highlight-blue {
	color: rgba(56, 125, 201, 1);
	fill: rgba(56, 125, 201, 1);
}
.highlight-purple {
	color: rgba(154, 107, 180, 1);
	fill: rgba(154, 107, 180, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(207, 81, 72, 1);
	fill: rgba(207, 81, 72, 1);
}
.highlight-default_background {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray_background {
	background: rgba(42, 28, 0, 0.07);
}
.highlight-brown_background {
	background: rgba(139, 46, 0, 0.086);
}
.highlight-orange_background {
	background: rgba(224, 101, 1, 0.129);
}
.highlight-yellow_background {
	background: rgba(211, 168, 0, 0.137);
}
.highlight-teal_background {
	background: rgba(0, 100, 45, 0.09);
}
.highlight-blue_background {
	background: rgba(0, 124, 215, 0.094);
}
.highlight-purple_background {
	background: rgba(102, 0, 178, 0.078);
}
.highlight-pink_background {
	background: rgba(197, 0, 93, 0.086);
}
.highlight-red_background {
	background: rgba(223, 22, 0, 0.094);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(125, 122, 117, 1);
	fill: rgba(125, 122, 117, 1);
}
.block-color-brown {
	color: rgba(159, 118, 90, 1);
	fill: rgba(159, 118, 90, 1);
}
.block-color-orange {
	color: rgba(210, 123, 45, 1);
	fill: rgba(210, 123, 45, 1);
}
.block-color-yellow {
	color: rgba(203, 148, 52, 1);
	fill: rgba(203, 148, 52, 1);
}
.block-color-teal {
	color: rgba(80, 148, 110, 1);
	fill: rgba(80, 148, 110, 1);
}
.block-color-blue {
	color: rgba(56, 125, 201, 1);
	fill: rgba(56, 125, 201, 1);
}
.block-color-purple {
	color: rgba(154, 107, 180, 1);
	fill: rgba(154, 107, 180, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(207, 81, 72, 1);
	fill: rgba(207, 81, 72, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(240, 239, 237, 1);
}
.block-color-brown_background {
	background: rgba(245, 237, 233, 1);
}
.block-color-orange_background {
	background: rgba(251, 235, 222, 1);
}
.block-color-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.block-color-teal_background {
	background: rgba(232, 241, 236, 1);
}
.block-color-blue_background {
	background: rgba(229, 242, 252, 1);
}
.block-color-purple_background {
	background: rgba(243, 235, 249, 1);
}
.block-color-pink_background {
	background: rgba(250, 233, 241, 1);
}
.block-color-red_background {
	background: rgba(252, 233, 231, 1);
}
.select-value-color-default { background-color: rgba(42, 28, 0, 0.07); }
.select-value-color-gray { background-color: rgba(28, 19, 1, 0.11); }
.select-value-color-brown { background-color: rgba(127, 51, 0, 0.156); }
.select-value-color-orange { background-color: rgba(196, 88, 0, 0.203); }
.select-value-color-yellow { background-color: rgba(209, 156, 0, 0.282); }
.select-value-color-green { background-color: rgba(0, 96, 38, 0.156); }
.select-value-color-blue { background-color: rgba(0, 118, 217, 0.203); }
.select-value-color-purple { background-color: rgba(92, 0, 163, 0.141); }
.select-value-color-pink { background-color: rgba(183, 0, 78, 0.152); }
.select-value-color-red { background-color: rgba(206, 24, 0, 0.164); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="7d9a6975-f78c-4951-8109-8215f84d661a" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">4️⃣</span></div><h1 class="page-title" dir="auto">Pytorch &amp; Neural Networks</h1><p class="page-description" dir="auto"></p></header><div class="page-body"><div style="display:contents" dir="auto"><p id="902f5182-1c3f-482e-99ac-bcf708919c27" class="">
</p></div><div style="display:contents" dir="auto"><p id="5b56a555-2ff7-4078-8683-1e3a920ae060" class="">This Lab-1 will introduce some of the basic concepts of the Pytorch library. How it will be used to build small neural networks and train then with a well developed dataset. Also at the end run some inferencing on the trained model to test its performance. Some of the concepts here will be discussed and covered during the lectures.</p></div><div style="display:contents" dir="auto"><p id="30ed8d36-62d4-46dc-bc74-4ac5c97f1c81" class="">
</p></div><div style="display:contents" dir="auto"><p id="31336016-116d-40f9-8d7b-0f872d8aff44" class="">Some of the best references are provided in the <mark class="highlight-blue"><strong><a href="Basic%20Coding%20Framework%20b5c7294a2be74534b1b7549d94d11b80.html">Prerequisites page</a></strong></mark>, please refer to the videos to learn the fundamentals. The official page for pytorch can be access from here <mark class="highlight-blue"><strong><a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Pytorch</a></strong></mark>.</p></div><div style="display:contents" dir="auto"><p id="a0eb5b4e-5d49-4e6e-8fb5-ad3fe12a3156" class="">
</p></div><div style="display:contents" dir="auto"><p id="6a382980-bed3-4817-b404-0ca09e1ad450" class="">A small introduction will be given on the quantization aspects and how to implement FP16, for the others reference links shall be provided for the students to support them in performing the requirements of Milestone-1.</p></div><div style="display:contents" dir="auto"><p id="17a3d192-8c64-8052-bbcb-e7b4ebe9c4f7" class="">
</p></div><div style="display:contents" dir="auto"><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Pytorch</summary><div class="indented"><div style="display:contents" dir="auto"><p id="189c6514-cc09-4e34-ae78-50c9a5a952ba" class="">Pytorch is a widely used opensource library working under the Python framework. Developed initially by Meta AI and now spearheaded by Linux group. The main focus of this library is to give the users support to build machine learning models and applications. </p></div><div style="display:contents" dir="auto"><p id="9aea7b09-ad93-4750-8ff7-564880addc58" class="">
</p></div><div style="display:contents" dir="auto"><p id="e611359e-83dd-49c8-9788-e8106c21cc4d" class="">Pytorch allows the script to be run on a Graphics processor or GPU. Which can speed up the math computations. (<em>ChatGPT was also trained on such 10,000 GPUs and the inferencing is also performed on them</em>.) In cases where there is no Cuda driver available (Nvidia GPUs), pytorch switches over to the cpu, or can direct it to use ROCm from the AMD GPUs.</p></div><div style="display:contents" dir="auto"><p id="8a101166-a3e6-4ff2-a17d-09f7f2b9c74a" class="">
</p></div><div style="display:contents" dir="auto"><p id="6c102a18-04d8-46a2-b84a-191a8dd034e4" class="">The data, arrays or matrices can be sent to different devices using pytorch for computation. This can be achieved with the inbuilt framework of moving the tensor data to different processors.</p></div><div style="display:contents" dir="auto"><p id="8b5dfa62-9c86-4dd6-9c46-57503e071e9c" class="">
</p></div><div style="display:contents" dir="auto"><ol type="1" id="4f72c3a6-eaa9-4b8a-97c5-53a4815c7e49" class="numbered-list" start="1"><li>Import Pytorch then create and operate on basic tensors.<div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ad4fd649-005d-4423-8717-3de85cf67494" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># Importing pytorch
import torch
import torch.nn as nn

# Generate an array of values between 0, 5 in step of 1
t1 = torch.linspace(0, 1, 5)
t2 = torch.arange(48).reshape(3,4,4)
print(&quot;t1&quot;, t1)
print(&quot;t2&quot;, t2)

# Broadcasting Rules, applying math operators on the tersor matrices
t3 = torch.ones((6,5))
t4 = torch.arange(5).reshape((1,5))
print(&quot;t3+t4\n&quot;, t3+t4)

# Other operation examples
t5 = torch.tensor([0.5, 1, 3, 4])
print(&quot;Mean&quot;, torch.mean(t5))
print(&quot;std&quot;, torch.std(t5))
print(&quot;max&quot;, torch.max(t5))
print(&quot;min&quot;, torch.min(t5))</code></pre></div></li></ol></div><div style="display:contents" dir="auto"><p id="babf04e7-d8a3-47bb-8eb5-952a02dfd4ae" class="">
</p></div><div style="display:contents" dir="auto"><ol type="1" id="aa43c475-2975-4dbe-b812-df5932645a9b" class="numbered-list" start="2"><li>There are many methods that are already predeveloped in this library, especially focusing on the requirements of image processing, machine learning, and neural networks.<div style="display:contents" dir="auto"><p id="210f9e31-42bb-44d5-811b-28fda9b5eae0" class="">
</p></div><div style="display:contents" dir="auto"><p id="37754074-5251-4cbf-b19a-b698104106d5" class="">Multiplying two matrices is very crucial in neural networks and another operation would be convolution. One such powerful method is computing of gradients. This is the core idea behind backpropagation and usage of loss functions.</p></div><div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="57535146-d600-49b7-a740-018ec251b8b7" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># Create a tensor matrix, and then add in the function (x^3)
x = torch.tensor([[5., 8.], [4., 6.]], requires_grad=True)
y = x.pow(3).sum()

# Now to compute the gradient or differentiate the function
comp_grad = y.backward()
print(&#x27;Gradient&#x27;, x.grad)

# Compare with the traditioal math result (3x^2)
diff_fx = 3*x**2
print(&#x27;Math result&#x27;, diff_fx)

# Performing matrix multplication
t1 = torch.randn((10, 10))
t2 = torch.randn((10, 10))
mat_mul = torch.matmul(t1, t2)
print(&#x27;Matrix Multiplication&#x27;, mat_mul)</code></pre></div></li></ol></div><div style="display:contents" dir="auto"><p id="df53a1be-b8df-4d20-b93b-41ead6ae68a8" class="">
</p></div></div></details></div><div style="display:contents" dir="auto"><p id="1793d192-8c64-8030-ba28-dc1fc0a7ff43" class="">
</p></div><div style="display:contents" dir="auto"><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Neural Networks</summary><div class="indented"><div style="display:contents" dir="auto"><p id="ef4e6567-08fd-4c92-a3dd-62bebfddc246" class="">Computer systems and architectures that mimic the connections of biological neurons. The basic element is a neuron, here its known as the perceptron, which can store a value/weight and then add some bias to the value when transferring the data. This system doesn’t mimic all the feature sets of a human brain, but it has given a new dimension in computing and recognizing patterns in large amounts of data.</p></div><div style="display:contents" dir="auto"><p id="c2758547-82e3-41d5-845d-1e1cd58cfb80" class="">
</p></div><div style="display:contents" dir="auto"><p id="0146439d-4f74-4668-b209-74318a15e47f" class="">Some of the models and methods were proposed more than 3 decades back, but the processor power and computational capability was limited. Hence took a back seat. With the advent of FinFET transistors giving more density and the Graphics processing with multi-core DSP the neural network based algorithms sprang back to life in the past decade. </p></div><div style="display:contents" dir="auto"><p id="557547bd-0dd5-40a8-b0af-62298b2b5d25" class="">
</p></div><div style="display:contents" dir="auto"><p id="e82d78a3-8b5e-45e9-b6ee-9ccac4256df8" class="">In this era we have also witnessed other forms of these neural networks for various applications. Some of them like RNN (Recurrent Neural Network), LSTM (Long Short Term Memory), GAN (Generative Adversarial Network), and Transformers are cornerstones for the pace of growth in the AI world.</p></div><div style="display:contents" dir="auto"><p id="65b7bde5-db48-4500-9b24-b38ff1528121" class="">
</p></div><div style="display:contents" dir="ltr"><figure id="a16cb54a-8dfd-4586-be25-24d9c80bd616" class="image" style="text-align:center"><a href="Untitled%205.png"><img style="width:624px" src="Untitled%205.png"/></a><figcaption>Input and weights representation <em>(Pablo Ruiz)</em></figcaption></figure></div><div style="display:contents" dir="auto"><p id="e63d5263-e6f1-4660-9b17-e363f7483853" class="">
</p></div><div style="display:contents" dir="ltr"><figure id="1f7ab6bc-59ac-468e-a73e-d18e7457e5c0" class="image" style="text-align:center"><a href="Untitled%206.png"><img style="width:624px" src="Untitled%206.png"/></a><figcaption>Multiplying the weight matrix and adding the bias vector to the input matrix <em>(Pablo Ruiz)</em></figcaption></figure></div><div style="display:contents" dir="auto"><p id="911819b9-9c27-480b-8d09-994f6cb10bd4" class="">
</p></div><div style="display:contents" dir="auto"><p id="958df010-daea-4994-ac6a-6c4b616b9fc5" class="">Here a simplistic implementation with a few linear layers and an hidden layer will be performed. The activation functions along with the loss function and backpropagation will be explored (<em>Parts of this is derived from </em><em><mark class="highlight-blue"><strong><a href="https://www.youtube.com/playlist?list=PLkdGijFCNuVk9fO1IMfdV1Igob0FUHhkB">Luke Polson’s</a></strong></mark></em><em> work</em>). Further in the next section an hands on for CNN (Convolutional Neural Network) is provided. </p></div><div style="display:contents" dir="auto"><p id="deecd609-4e70-4d5d-86e2-19886545ba3e" class="">Another good reference to learn to build neural network and understand their intricacies <mark class="highlight-blue"><strong><a href="https://www.youtube.com/playlist?list=PLPTV0NXA_ZSj6tNyn_UadmUeU3Q3oR-hu">Build Neural Networks from Scratch</a></strong></mark><mark class="highlight-default">. Developed by PhD graduates from MIT.</mark></p></div><div style="display:contents" dir="auto"><p id="53d996f9-80d2-420b-b4d6-e12fbbbdc9a6" class="">
</p></div><div style="display:contents" dir="auto"><ol type="1" id="0c0b91de-499e-4509-9b44-613c474f6713" class="numbered-list" start="1"><li>Create a data set for the inputs and validation for comparing the output. Consider a matrix 2x4 which as X and it maps to the outputs 1x4. This relationship between the input and the output will be learnt by the neural network. </li></ol></div><div style="display:contents" dir="auto"><ol type="1" id="833c5955-8b1b-456a-ad9c-c07b25b73278" class="numbered-list" start="2"><li>Two linear layers are implemented here. Pytorch library has the built-in “nn” module which can be invoked to create the layers of the network. The weights of this network is decided randomly initially.<div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="03fcbbd0-2bd7-40d0-857e-241b6fb2e3c5" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np                 # For matrix computation
import matplotlib.pyplot as plt    # For plotting

# Create the input and validation data
x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()
y = torch.tensor([1,5,2,5]).float()

# Neural network with two linear layers
class tryNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.mat1 = nn.Linear(2,8,bias=False)
        self.mat2 = nn.Linear(8,1,bias=False)
        
    # Here mat x is the input data
    def forward(self, x):
        x = self.mat1(x)
        x = self.mat2(x)
        return x.squeeze()
        
network_nn = tryNN()
y_computed = f(x)
print(&#x27;y_computed by NN:&#x27;, y_computed)
print(&#x27;Original validation output Y:&#x27;, y)</code></pre></div></li></ol></div><div style="display:contents" dir="auto"><p id="87528af3-eac9-4455-b992-af3f359c32ff" class="">
</p></div><div style="display:contents" dir="auto"><ol type="1" id="d1441f07-664c-4154-baca-1ebff4fd5757" class="numbered-list" start="3"><li>Now to adjust the computed Y to get closer to the original output required, a loss function is computed which here is a mean square error a statistical method to compare the closeness of the generated data to the required data.<div style="display:contents" dir="auto"><p id="e11ad01c-a645-42c5-9765-e33080703e83" class="">
</p></div><div style="display:contents" dir="auto"><p id="650c50d8-0cf5-46c2-be1b-ff24f027625b" class="">Once the loss function is computed the next step would be to adjust the loss based on Stochastic Gradient Descent, where the goal is to reduce the loss in the subsequent iterations (or Epochs). </p></div><div style="display:contents" dir="auto"><p id="a18117ce-5e12-477d-b146-3d4cd6dbe09f" class="">
</p></div><div style="display:contents" dir="auto"><p id="b9b3b312-f456-440b-810d-8c20509e8b90" class="">Here the learning rate will determine how fast the network learns, or how slow with better update. If the learning rate is too slow we’ll have a vanishing gradient where the next change or loss would be too small to distinguish with the chain derivative.</p></div><div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="27857e33-c011-4f54-8c5f-88a6baed5ad1" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># Loss funcitons SSR
L = nn.MSELoss()
L(y, y_computed)

# Optimizer SGD
opt = torch.optim.SGD(f.parameters(), lr=0.001)

# Epochs
losses = []
for epochs in range(50):
    opt.zero_grad()
    loss_value = L(f(x), y)
    loss_value.backward()
    opt.step()
    losses.append(loss_value.item())

y_computed = f(x)
print(&#x27;y_computed by NN:&#x27;, y_computed)
print(&#x27;Original validation output Y:&#x27;, y)

# Plot the loss vs epoch
plt.plot(losses)
plt.xlabel(&#x27;Epoch&#x27;)
plt.ylabel(&#x27;Loss&#x27;)</code></pre></div></li></ol></div><div style="display:contents" dir="auto"><ol type="1" id="e9c98346-4170-417a-8deb-3a8e3677b5d8" class="numbered-list" start="4"><li>The results are not that accurate, to improve the performance, additional activation function layer is used with an hidden layer. Here the method to save of the model is shown. Here the nodes in the linear layer is increased to 2x80 and 80x1. And adding an hidden layer of 80x80. The training is performed with 5000 epochs and 0.001 learning rate.<div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="43db11b0-ff32-41f9-bcbd-8f39e43330cd" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np                
import matplotlib.pyplot as plt

# Device GPU or CPU
device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)

# Create the input and validation data
x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()
y = torch.tensor([1,5,2,5]).float()

class tryNN2(nn.Module):
    def __init__(self):
        super().__init__()
        self.mat1 = nn.Linear(2,80,bias=False)
        self.mat2 = nn.Linear(80,80)
        self.mat3 = nn.Linear(80,1,bias=False)
        self.relu = nn.ReLU()
        
    # Here mat x is the input data
    def forward(self, x):
        x = self.mat1(x)
        x = self.relu(x)
        x = self.mat2(x)
        x = self.relu(x)
        x = self.mat3(x)
        return x.squeeze()

# training
def train_model(x, y, f, n_epoch):
    opt = torch.optim.SGD(f.parameters(), lr=0.001)
    L = nn.MSELoss()
    
    losses = []
    for epochs in range(n_epoch):
        opt.zero_grad()
        loss_value = L(f(x), y)
        loss_value.backward()
        opt.step()
        losses.append(loss_value.item())
    return f, losses

# Data input and labels
x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float().to(device)
y = torch.tensor([1,5,2,5]).float().to(device)
f2 = tryNN2().to(device)

# Training
f2, losses = train_model(x, y, f2, n_epoch=5000)
y_computed = f2(x)
print(&#x27;y_computed by NN:&#x27;, y_computed)
print(&#x27;Original validation output Y:&#x27;, y)

# Saving the trained model
quant_model = tryNN2().to(device)
torch.save(quant_model.state_dict(), &#x27;linear_nn_relu.pt&#x27;)</code></pre></div></li></ol></div></div></details></div><div style="display:contents" dir="auto"><p id="1793d192-8c64-80bb-bfb1-d8cdf9892fc0" class="">
</p></div><div style="display:contents" dir="auto"><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Convolutional Neural Networks</summary><div class="indented"><div style="display:contents" dir="auto"><p id="d399d3cc-beac-4301-ad48-5a1c0f0c2d26" class="">Convolution in essence is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. This is an operation performed on a matrix of values with another matrix called the kernel.</p></div><div style="display:contents" dir="auto"><p id="65b7ceb7-8683-4e6e-ad99-77554e95399e" class="">
</p></div><div style="display:contents" dir="auto"><p id="6974b46a-8c9a-41fc-a79c-e775a3e4cb9b" class="">One of the effective feature of performing convolution on image data is extracting the features. Manually creating filters to find edges and other feature sets to identify an object is an uphill challenge. Added to this for each object the parameters change, and also based on it spatial variations. This is where convolution operations with kernel matrices with multiple stage can bifurcate and learn the features in the image.</p></div><div style="display:contents" dir="auto"><p id="b554ac56-3912-4a28-9629-1da789fcda6b" class="">
</p></div><div style="display:contents" dir="ltr"><figure id="63ce8c65-35e6-423b-b10d-4eb7ad7ee1ee" class="image" style="text-align:center"><a href="Untitled%207.png"><img style="width:576px" src="Untitled%207.png"/></a><figcaption>Convolution performed with the Kernel and resulting matrix - <em>(IBM)</em></figcaption></figure></div><div style="display:contents" dir="auto"><p id="37e38d45-4021-4a8e-bf33-a8601ce15998" class="">
</p></div><div style="display:contents" dir="auto"><p id="e8d6dc13-9e69-410d-b43d-634770714bc9" class="">Additional support is provided by the activation functions and fully connected layers in the subsequent steps, which help in classifying the image based on the features detected. Between convolution operations, pooling is performed to reduce the dimension of the data and retain as much as possible. This pooling is majorly of two types, averaging and maximum. Where a kernel of values are selected from the data and then the average or maximum value of the kernel is retained for further operations.</p></div><div style="display:contents" dir="auto"><p id="d3ddd6a0-bc90-40eb-aefd-465f8a4276a3" class="">
</p></div><div style="display:contents" dir="ltr"><figure id="d46c427f-4fd1-4f76-b0df-88e06ef7bbbb" class="image" style="text-align:center"><a href="Untitled%208.png"><img style="width:384px" src="Untitled%208.png"/></a><figcaption>Pooling after convolution operation in LeNet-5</figcaption></figure></div><div style="display:contents" dir="auto"><p id="6977681d-916a-461f-b0d4-fa7a6a066fcd" class="">
</p></div><div style="display:contents" dir="auto"><p id="9e6ba727-473d-4363-a505-84ff7279b7d0" class="">References to learn are provided in the <mark class="highlight-blue"><strong><a href="Basic%20Coding%20Framework%20b5c7294a2be74534b1b7549d94d11b80.html">Prerequisites</a></strong></mark><mark class="highlight-default">, to learn more about CNN. An implementation of CNN is done in this lab session, with the popular LeNet-5 architecture having 5 layers comprising of the two 2D-convolution layers.</mark></p></div><div style="display:contents" dir="auto"><p id="e1f31e2e-8438-4305-a465-64423e6a301d" class="">
</p></div><div style="display:contents" dir="ltr"><figure id="d168fa3e-d7e4-4dc3-ab52-f40f9d1a6013" class="image" style="text-align:center"><a href="Untitled%209.png"><img style="width:624px" src="Untitled%209.png"/></a><figcaption>LeNet-5 architecture - <em>(LeChunn, Fukushiwa, 1998)</em></figcaption></figure></div><div style="display:contents" dir="auto"><p id="0eca5934-66ab-4c77-a6ac-d66ea2a62fa0" class="">
</p></div><div style="display:contents" dir="auto"><p id="d2c5754f-4f8f-4d78-96fa-2dab60ec075d" class="">Training the LeNet-5 network on the standard MNIST handwritten dataset. The dataset should be downloaded from an online archive, similarly for other datasets as well <em>(Fashion MNIST, CIFAR10, &amp; CIFAR100)</em>. The other advantage is this dataset is cleaned and statistically reliable for the neural network training, in situations where the data is self created, additional steps to clean and label the data has to be performed. </p></div><div style="display:contents" dir="auto"><p id="9a44af23-8943-4f7c-9c23-22c0cd70f16a" class="">
</p></div><div style="display:contents" dir="auto"><p id="0fec0887-f306-4532-8c0e-f0de7d29a84f" class="">The dataset downloaded is divided into two sets one for training and another for validation. The MNIST data contains the image of handwritten digits along with the labels, in the real data one creates, labelling is an important without which training doesn’t yield results. Also there would be no benchmark to validate against.</p></div><div style="display:contents" dir="auto"><p id="e033ca8c-5f2a-4afc-8b00-46f3365c7a08" class="">
</p></div><div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e4fa3fb7-e45d-41ac-a09c-d800bbee11fb" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torch
import torchvision
import time
import os
import numpy as np

import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import matplotlib.pyplot as plt


device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)

# Control parameters
l_r = 0.001
n_epoch = 5
n_datasets = 64
n_classes = 10

# Load the data set for training and testing
# Can be loaded from the server or local

# Applying transform on the input data, rezise to 32x32, convert to tensor, normalize the data
transform_on_train_data = transforms.Compose([transforms.Resize((32,32)),
                                              transforms.ToTensor(),
                                              transforms.Normalize(mean=(0.1307,), std=(0.3081,))])
                                            
transform_on_test_data = transforms.Compose([transforms.Resize((32,32)),
                                            transforms.ToTensor(),
                                            transforms.Normalize(mean=(0.1325,), std=(0.3105,))])

# Loading the training data and test data to/from path, download will control the action
# For training data it is 1, for downloading from server it is 1
data_train = torchvision.datasets.MNIST(root=&#x27;C:/Users/vivek/Documents/GitHub/Python_Physics/stat_quest/mnist&#x27;,
                                        train=True,
                                        transform=transform_on_train_data,
                                        download=False)

data_test = torchvision.datasets.MNIST(root=&#x27;C:/Users/vivek/Documents/GitHub/Python_Physics/stat_quest/mnist&#x27;,
                                       train=False,
                                       transform=transform_on_test_data,
                                       download=False)

# Loading the training and test data according to requirement / parameters
# Load in the dataset size / batches required
# To shuffle the data sequence
load_data_train = torch.utils.data.DataLoader(dataset=data_train,
                                              batch_size=n_datasets,
                                              shuffle=True)
load_data_test = torch.utils.data.DataLoader(dataset=data_test,
                                             batch_size=n_datasets,
                                             shuffle=True)</code></pre></div><div style="display:contents" dir="auto"><p id="9a40982c-23eb-4a92-a6e6-e1cd8f02d0ec" class="">
</p></div><div style="display:contents" dir="auto"><p id="93cf069d-ef05-4967-a328-f4d62a7f7e87" class="">→ CNN with LeNet-5</p></div><div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bd0069b0-6146-4fe0-a707-01b06fe3d07d" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#Defining the convolutional neural network
class LeNet5_mnist(nn.Module):
    def __init__(self, num_classes):
        super(ConvNeuralNet, self).__init__()
        self.layer1 = nn.Sequential(
	            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),
		          nn.BatchNorm2d(6),
	            nn.ReLU(),
	            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.layer2 = nn.Sequential(
	            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),
	            nn.BatchNorm2d(16),
	            nn.ReLU(),
	            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.fc = nn.Linear(400, 120)
        self.relu = nn.ReLU()
        self.fc1 = nn.Linear(120, 84)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(84, num_classes)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        out = self.relu(out)
        out = self.fc1(out)
        out = self.relu1(out)
        out = self.fc2(out)
        return out
        
# Hyperparameter tuning
model = LeNet5(num_classes).to(device)
cost = nn.CrossEntropyLoss()           #Setting the loss function

#Setting the optimizer with the model parameters and learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#this is defined to print how many steps are remaining when training
total_step = len(train_loader)</code></pre></div><div style="display:contents" dir="auto"><p id="ad4c8f96-e1c2-4d3c-89db-647f021d37f0" class="">
</p></div><div style="display:contents" dir="auto"><p id="88a2bb65-106d-4946-ace0-2eea44f62758" class="">→ Training and Validation</p></div><div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f1994286-3012-4c80-8476-0bbd67f3db43" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># Traning and keep track of each epoch
total_step = len(load_data_train)
for epoch in range(n_epoch):
    for i, (images, labels) in enumerate(load_data_train):  
        images = images.to(device)
        labels = labels.to(device)
        
        #Forward pass
        outputs = model(images)
        loss = cost(outputs, labels)
        	
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        		
        if (i+1) % 400 == 0:
            print (&#x27;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#x27; 
        		           .format(epoch+1, n_epoch, i+1, total_step, loss.item()))
        		           
# Saving the trained model
quant_model = LeNet5_mnist().to(device)
torch.save(quant_model.state_dict(), &#x27;CNN_mnist.pt&#x27;)

# Validate the model, check the accuracy of the trained model
# In test phase, we don&#x27;t need to compute gradients (for memory efficiency)
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in load_data_test:
        images = images.to(device)
        labels = labels.to(device)
        
        # Input the images to the trained model and store the outputs, then validate
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    print(&#x27;Accuracy of the network on the 10000 test images: {} %&#x27;.format(100 * correct / total))
	 </code></pre></div></div></details></div><div style="display:contents" dir="auto"><p id="1793d192-8c64-80d0-b919-e76d608f8d33" class="">
</p></div><div style="display:contents" dir="auto"><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Quantization</summary><div class="indented"><div style="display:contents" dir="auto"><p id="d7dc8eed-22ba-4f8b-a1f5-7e3a34ebad68" class="">The datatype of the dataset and the model is default “float32”.  The GPUs on the server can support single precession (float32) and double precession (float64). To optimize the training for other hardware platforms the training will be performed with quantized data and model. </p></div><div style="display:contents" dir="auto"><p id="c1ca5ae5-38b9-49eb-848f-6713d0a58c22" class="">
</p></div><div style="display:contents" dir="auto"><p id="f1943008-b828-43ac-8e72-2b2783da1ba7" class="">Based on the requirement and the hardware accessibility the quantization can be performed. There are various in-built methods in Pytorch to perform this post training, or during the training process, and complete quantization of the dataset and the model being trained. Further references to this can be accessed from the official documentation and tutorial page of Pytorch website. <em>(Refer the links below).</em></p></div><div style="display:contents" dir="auto"><ol type="1" id="48bb0571-20b8-4c93-97c8-a80724719a96" class="numbered-list" start="1"><li><mark class="highlight-blue"><strong><a href="https://pytorch.org/docs/stable/quantization.html">Quantization in Pytorch</a></strong></mark></li></ol></div><div style="display:contents" dir="auto"><ol type="1" id="af9740a1-8426-4c41-a7dd-329eb91ae922" class="numbered-list" start="2"><li><mark class="highlight-blue"><strong><a href="https://pytorch.org/tutorials/recipes/quantization.html">Quantization Recipe</a></strong></mark></li></ol></div><div style="display:contents" dir="auto"><ol type="1" id="55b0a31b-7f23-4584-b4cb-97bb41685644" class="numbered-list" start="3"><li><mark class="highlight-blue"><strong><a href="https://www.youtube.com/watch?v=0VdNflU08yA">Detailed analysis on Quantization with Pytorch</a></strong></mark></li></ol></div><div style="display:contents" dir="auto"><p id="5a335270-0f25-4f77-a86d-61e408dac58c" class="">
</p></div><div style="display:contents" dir="auto"><p id="d7600e80-1686-4b8d-b340-7c8fb7342353" class="">Quantization directly helps in reducing the hardware requirements, computing power and even memory. This directly improves the power consumption and the area consumed on chip or in servers. But there will be some drawback in terms of the accuracy of the model. This can be accommodated based on the degree of precession and requirements of the application system.</p></div><div style="display:contents" dir="auto"><p id="d29fa123-0167-4b34-bb58-964858a2e484" class="">
</p></div><div style="display:contents" dir="auto"><p id="602a86a3-e73f-4634-ab3c-8a345bb841e8" class="">This is very much employed in embedded based edge computing and neuromorphic computing which is discussed in this course. <mark class="highlight-red"><strong><a href="https://www.tinyml.org/">TinyML</a></strong></mark><mark class="highlight-red"><strong> </strong></mark>is one such opensource platform for implementing ML algorithms on microcontroller and remote battery operated processors. </p></div><div style="display:contents" dir="auto"><p id="3f839703-a62a-4166-b1f4-0be5e4862024" class="">
</p></div><div style="display:contents" dir="auto"><p id="e1712244-286b-4e07-a3cb-39a91eea4d5a" class="">There are methods where the model can be trained on single precession, and then can be used for validation in integer datatype. Another possibility is training and validating the model in floating point, and then inferencing in integer type. In such cases the model is retrained on the embedded or lower precession system to have better adaptation and accuracy in inferencing and validation. </p></div><div style="display:contents" dir="auto"><p id="5fe19bc4-8264-4d64-b4a1-89c341414211" class="">
</p></div><div style="display:contents" dir="auto"><p id="3b3b6783-23aa-4672-b7f7-5480fcf6d503" class="">These aspects can be explored further by the student, and a more detailed discussion will take place along with the lectures regarding these topics during the course.</p></div><div style="display:contents" dir="auto"><p id="935fa5b2-b9e0-4085-afe9-c05fefc94412" class="">
</p></div><div style="display:contents" dir="auto"><p id="a6d60d94-99d3-403b-a06a-50731729d4cc" class="">Quantization from “float32” to “float16” a half precession datatype. Here the complete model, training and validation is performed in float16. This will improve the speed of training and reduce the memory resources consumed along with the size of the model. But the accuracy will drop slightly, it is a always a trade off. </p></div><div style="display:contents" dir="auto"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c7f78068-b00c-4f16-9015-eb0e245f499d" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># Setting the datatype to FP16 (Quantization)
# Converting the model to half precession (FP16)
# Loading the model to the particular device (CPU / GPU)
torch.set_default_dtype(torch.float16)
model = model.LeNet5_mnist(Parameter_nn.n_classes).to(device)
model_load = model.half()</code></pre></div><div style="display:contents" dir="auto"><p id="415fcf70-a405-46f3-83a2-19c165f0780d" class="">
</p></div><div style="display:contents" dir="auto"><p id="de9101f8-1eab-46a4-98ad-cbf171abb606" class=""><em>Add this code snippet after defining the neural network LeNet5_minist in the previous script written for the CNN implementation. Remember to add this before call the training epochs method or the validation method. This ensures the model is quantized to FP16 from default FP32.</em></p></div><div style="display:contents" dir="auto"><p id="b256b50e-f147-4aaf-847a-51369dff2d2c" class="">
</p></div></div></details></div><div style="display:contents" dir="auto"><p id="8409dd07-6276-4e64-9936-b894b1d383e4" class="">
</p></div><div style="display:contents" dir="auto"><p id="cc367779-ead3-40dd-a686-1100ce8aa1cc" class="">
</p></div><div style="display:contents" dir="auto"><p id="e708b904-3aa7-4ef0-96f6-c1440354a351" class="">
</p></div><div style="display:contents" dir="auto"><p id="4c96d21e-0945-4717-9394-435128de40df" class="">
</p></div></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>